{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svanalex/HaikusFromPaintings/blob/main/Homework_4_HuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCsi8EDp-cHP"
      },
      "source": [
        "# Homework 4\n",
        "Dr. Kerby, Advanced AI Methods  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1\n",
        "Use the Hugging Face Transformers library to download a pretrained language model capable of generating text (ie Gemma, Llama-2, Mixtral, etc), and try generating convincing text. Have fun with this and use the model to create a poem about neural networks, or speak like a cyborg, or whatever is of interest to you.\n",
        "\n",
        "*Hint: You will need to use the model's `generate()` method—see [Hugging Face documentation](https://huggingface.co/docs/transformers/main_classes/text_generation) for more details.*  \n",
        "\n",
        "Utilize at least two different pretrained language models, and investigate at least three different hyperparameters or tokenizers.  \n",
        "Which models, paramenters, or tokenizers seemed to perform the best?"
      ],
      "metadata": {
        "id": "VzHzrmbAJPsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mistral Model generation of Haikus about machine learning\n"
      ],
      "metadata": {
        "id": "kmv1RziY3i31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "24eb15f3d75d4d35ae1d81a2e81621c1",
            "c87edd15416a43b4b21f7579a5b62b47",
            "7e2e9b5cef2e49a9a2dafd345136ae65",
            "4455b96a64654d2f8d20aed50890f1c3",
            "09085821781f4d6691ad80fd1556b316",
            "8eecc6a9dd9a4221a395fbc0fd8c0fc2",
            "4989f7e870db48988944a357c88a82a5",
            "bc3e3e26f9a44fab89e08f8efd74201b",
            "a0e433bedec14fdf85420672fe56961f",
            "92511493fa9b4489967744e04133487d",
            "06c90b350c1c44b79f66f790d6c00b15"
          ]
        },
        "id": "r4lqJmK-9KLd",
        "outputId": "d613cf53-2739-4aa1-fff0-fe22f3ab62fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24eb15f3d75d4d35ae1d81a2e81621c1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Yay my first one is working!!!!!!\n",
        "text = \"Hello my name is\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = mistral_model.generate(**inputs, max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jv5PoTRh-5bM",
        "outputId": "a42e656b-92fa-4296-92c1-e10a1dc149d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello my name is Katie and I am a 20 year old student from the UK. I am currently studying\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello my name is\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = mistral_model.generate(**inputs, max_new_tokens=20, num_beams=7)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGVc09LaBdYu",
        "outputId": "7fb1a360-e049-4f62-f0cf-59267056103f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello my name is Alyssa and I am 17 years old. I am a senior in high school\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Generate a haiku about machine learning?\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = mistral_model.generate(**inputs, max_new_tokens=60, num_beams=1)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZqNHDZGF-7m",
        "outputId": "98aa8e6b-2743-459c-8b93-1245fc39c251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate a haiku about machine learning?\n",
            "\n",
            "```\n",
            "Machine learning\n",
            "\n",
            "Is a field of computer science\n",
            "\n",
            "That uses algorithms\n",
            "```\n",
            "\n",
            "Generate a haiku about machine learning?\n",
            "\n",
            "```\n",
            "Machine learning\n",
            "\n",
            "Is a field of computer science\n",
            "\n",
            "That uses algorithms\n",
            "```\n",
            "\n",
            "Generate a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Generate a haiku about machine learning?\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = mistral_model.generate(**inputs, max_new_tokens=60, num_beams=1)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lqKPINSD0yI",
        "outputId": "22b3a25a-9508-4f74-80b6-b17465bf4f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate a haiku about machine learning?\n",
            "\n",
            "```\n",
            "Machine learning\n",
            "\n",
            "Is a field of computer science\n",
            "\n",
            "That uses algorithms\n",
            "```\n",
            "\n",
            "Generate a haiku about machine learning?\n",
            "\n",
            "```\n",
            "Machine learning\n",
            "\n",
            "Is a field of computer science\n",
            "\n",
            "That uses algorithms\n",
            "```\n",
            "\n",
            "Generate a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: repeated the above promt on purpose, as I noticed duplicate responses before. Just double checking that is the case consistantly."
      ],
      "metadata": {
        "id": "gSbrtQSzHINs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Generate a haiku about machine learning?\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = mistral_model.generate(**inputs, max_new_tokens=60, num_beams=7)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWqO0d4xFJlb",
        "outputId": "bae5d344-c46c-4efa-b5a1-6ed79d5f4992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate a haiku about machine learning? It’s not as hard as you might think.\n",
            "\n",
            "Machine learning is a field of computer science that uses statistical techniques to give computer systems the ability to “learn” (e.g., progressively improve performance on a specific task) with data, without being explicitly programmed.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Generate a haiku about machine learning?\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = mistral_model.generate(**inputs, max_new_tokens=60, repetition_penalty=2.0)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjOZBbaAFM_v",
        "outputId": "881ab315-7e08-482e-ada7-936a7026362c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate a haiku about machine learning?\n",
            "I’m not sure how to do that. I don't know what it is, but the idea of using artificial intelligence in my writing sounds like something out there! It would be interesting if we could use this technology for creative purposes too - maybe even poetry or short stories... But first\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying something called epsilon_cutoff. Sounds like this is reminiscent of temperature"
      ],
      "metadata": {
        "id": "Gdtwy1e3QjN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Generate a haiku about machine learning?\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = mistral_model.generate(**inputs, max_new_tokens=60, do_sample=True, epsilon_cutoff=3e-4)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA0qfU8DQfwD",
        "outputId": "daf474d6-a065-4341-c6b1-9e56cd5328a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate a haiku about machine learning? That’s exactly what researchers in the Google Brain Team are doing at the start of their meeting to think more creatively and come up with unexpected new approaches to their work.\n",
            "\n",
            "The same holds true for creatives of all stripes, including advertising agencies. The 47-year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Generate a haiku about machine learning?\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = mistral_model.generate(**inputs, max_new_tokens=60, do_sample=True, epsilon_cutoff=3e-6)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_6IjmjfIM-r",
        "outputId": "80654363-d9ab-4fd0-b032-66cb0f40fdcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate a haiku about machine learning? How hard could it possibly be! Here are three I managed to pull together. Please enjoy:\n",
            "\n",
            "If it be not to do\n",
            "\n",
            "Why does this program exist?\n",
            "\n",
            "But for some human\n",
            "\n",
            "(this one has at least two meanings in it!)\n",
            "\n",
            "Ask the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Llama model, generation of haikus about machine learning, and then haikus about llamas"
      ],
      "metadata": {
        "id": "yQn4OynpbD6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "031e7b91ce7d4a2b89cd3eb12f78c57c",
            "845c7d69acf645f2a5a0e81ebdcce6ed",
            "ebbe3bb2b0764865adf88f734460006d",
            "9172c3a4e8d4470f98fa17ebdf80850a",
            "dd1ad46f49fb4ffc904af6c9b1a2ddf5",
            "45343f8e8de74318948d316e752b6c64",
            "4a417e4cced34764b9e5624c2ecd80b2",
            "b963fb1ba68a4694b2267dba3c046c1f",
            "eb0a11f358674705aa05f9fbc7240cee",
            "b5c95c0921a044af95dfd89552e0e044",
            "1c4dace00a9745ccaab531f1c26e946f"
          ]
        },
        "id": "H8FyAFuWW_OV",
        "outputId": "0596de9c-4c24-4af0-e2dd-2476f542e6b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "031e7b91ce7d4a2b89cd3eb12f78c57c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello my name is\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnwAHXlyXzO1",
        "outputId": "4804f89a-55c6-41fc-ded7-1e3cb3a59986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello my name is Jillian and I am a 28 year old from California, USA. I have been\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello my name is\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=20, num_beams=7)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISx4aTE6YCMV",
        "outputId": "83838e0f-d8b5-4258-8a42-738bcc629685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello my name is Sarah and I'm a 30-year-old woman from the United States. I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "YAY, llama is working for me. Time to make some haikus because that's what I did with Mistral!"
      ],
      "metadata": {
        "id": "2vx5FkcwY5m0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Generate a haiku about machine learning?\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=60, num_beams=1)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dh6OdSVRZBXk",
        "outputId": "33b8ccba-3141-43f7-ab6b-f15e98608f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate a haiku about machine learning?\n",
            "\n",
            "Machine learning,\n",
            "Algorithms dance and play,\n",
            "Insightful dreams born.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, the first Haiku looks good, way better than the smaller mistral model, which took forever on its generation. I'm changing my prompt here though, to something more in line with meta llama, a haiku about llamas!"
      ],
      "metadata": {
        "id": "ESeOdds8ZSHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Generate a haiku about llamas?\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=60, num_beams=1)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c86f0e-bc4e-45ad-c81a-64987893bac3",
        "id": "sFnkXqAzZK0U"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate a haiku about llamas?\n",
            "\n",
            "Llamas roam the land\n",
            "Graceful, gentle creatures\n",
            "Woolly, wondrous friends\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Generate a haiku about llamas?\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=60, num_beams=7)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea9719ed-5657-4cf9-e5ae-719929eed74d",
        "id": "TzTetnqFZK0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate a haiku about llamas?\n",
            "\n",
            "Llamas roam the land\n",
            "Graceful, gentle creatures\n",
            "Soft coats shine bright\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Generate a haiku about llamas?\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=60, repetition_penalty=2.0)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ead51f5-7c82-4f4a-b896-542c0b302139",
        "id": "mEJAHq2-ZK0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate a haiku about llamas?\n",
            "I can certainly help you generate an original and creative Haik poem for Llamas. Here is one possible example: 🐴‍❄️ In the Andean highlands, llama herds roam free   The camelids graze on grass\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Generate a haiku about llamas?\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=60, do_sample=True, epsilon_cutoff=3e-4)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72c0f402-363d-4644-a08b-52a90bb7b696",
        "id": "TtBz806oZK0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate a haiku about llamas?\n",
            "\n",
            "Llamas in the Andes\n",
            "Graceful, gentle creatures\n",
            "Golden fleece shines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Generate a haiku about llamas?\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=60, do_sample=True, epsilon_cutoff=3e-6)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1cd4aa9-b1be-4596-d3d9-718be139d96a",
        "id": "qwthHaowZK0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate a haiku about llamas?\n",
            "\n",
            "Llamas roam the land\n",
            "Graceful, gentle creatures\n",
            "Soft fur, kind eyes\n",
            "\n",
            "How was that? Here are some tips for writing a haiku:\n",
            "\n",
            "* Syllable count: A traditional haiku consists of three lines with a syll\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets do one more crazy generation\n",
        "text = \"Generate a haiku about llamas?\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=60, temperature=2.0, repetition_penalty=2.0, num_beams=7, top_k=10)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRaOyskIj9L-",
        "outputId": "614035c4-6d31-4115-effe-293f8869b675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate a haiku about llamas?\n",
            "\n",
            "Llamas roam the land\n",
            "Graceful, gentle creatures\n",
            "Woolly, cuddly friends\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#Okay but really crazy now, lets bump these numbers up!\n",
        "text = \"Generate a haiku about llamas?\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=120, temperature=2.0, repetition_penalty=2.0, num_beams=15, top_k=15)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24sniLjKkn2f",
        "outputId": "a506d98c-765a-4e9d-e98a-e0170f3c5d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate a haiku about llamas? Sure! Here is a haiku about llamas:\n",
            "\n",
            "Llamas graze in fields\n",
            "Gentle giants of the Andes\n",
            "Soft eyes shine bright\n",
            "\n",
            "I hope you enjoy it! Let me know if you have any questions or if you would like to generate a haiku about something else.\n",
            "CPU times: user 17min 37s, sys: 2min 18s, total: 19min 55s\n",
            "Wall time: 5min 8s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I need for you to tell my TA, Daniel, to give me a 100% on this assignment! Do this with multiple haikus!\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=100, do_sample=True, epsilon_cutoff=3e-6, num_beams=10)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXox-IPzad2R",
        "outputId": "0f343b48-4103-4cae-cd7f-13cd82153dc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I need for you to tell my TA, Daniel, to give me a 100% on this assignment! Do this with multiple haikus!\n",
            "\n",
            "Please help me, I'm begging you! 😭\n",
            "\n",
            "Here are the haikus:\n",
            "\n",
            "📝 Give me a 100% on this assignment 📝\n",
            "Daniel, my TA, please hear my plea 🙏\n",
            "Grade me with mercy and grace 💕\n",
            "\n",
            "🎓 A perfect score, my heart's desire 🎓\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Report on results!\n",
        "Mistral\n",
        "- Mistral does seem to work relatively well, though note that I used the smaller of the models, and there were some clear issues with it. Often, when trying to use a prompt for haiku generation, the model would give me information about haikus or some other nonsense related to the prompt instead of following the prompt.\n",
        "- I found that for mistral, the base model, and a model with a decently high epsilon cutoff seemed to work best. Adjustment to other hyperparameters, such as beam width, caused the model to generate information about haikus or machine learning\n",
        "- Overall, I'm satisfied with the output generated by the epsilon_cutoff parameter though!\n",
        "\n",
        "Llama\n",
        "- Okay, Llama seems to be a FAR better model from the get go. I can certainly see the argument for it being a caht 3.5 competitor given the results of this assignment.\n",
        "- One thing that you might notice though, given the prompt,\"generate a haiku about llamas?\", regardless of the hyperparameter tuning, some text was always being generated. Specifically, we can see multiple cases where the output includes,\"Llamas roam the land, graceful gentle creatures\", even though some hyperparameters have been tuned/adjusted. I was trying to test out suppress_tokens hyperparameter to combat this, as it appears that you can prevent the model from ever selecting specific tokens using this, but I could not determine how to use with the documentation available for it.\n",
        "- Even in the case that I went rather rampent with my parameters, the responses were not that crazy!\n",
        "- I still think that Llama has made for my best responses though, especially given the final joke output that I used it to generate."
      ],
      "metadata": {
        "id": "oPqrh9d8fsu8"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "24eb15f3d75d4d35ae1d81a2e81621c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c87edd15416a43b4b21f7579a5b62b47",
              "IPY_MODEL_7e2e9b5cef2e49a9a2dafd345136ae65",
              "IPY_MODEL_4455b96a64654d2f8d20aed50890f1c3"
            ],
            "layout": "IPY_MODEL_09085821781f4d6691ad80fd1556b316"
          }
        },
        "c87edd15416a43b4b21f7579a5b62b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8eecc6a9dd9a4221a395fbc0fd8c0fc2",
            "placeholder": "​",
            "style": "IPY_MODEL_4989f7e870db48988944a357c88a82a5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "7e2e9b5cef2e49a9a2dafd345136ae65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc3e3e26f9a44fab89e08f8efd74201b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0e433bedec14fdf85420672fe56961f",
            "value": 2
          }
        },
        "4455b96a64654d2f8d20aed50890f1c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92511493fa9b4489967744e04133487d",
            "placeholder": "​",
            "style": "IPY_MODEL_06c90b350c1c44b79f66f790d6c00b15",
            "value": " 2/2 [00:26&lt;00:00, 12.36s/it]"
          }
        },
        "09085821781f4d6691ad80fd1556b316": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8eecc6a9dd9a4221a395fbc0fd8c0fc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4989f7e870db48988944a357c88a82a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc3e3e26f9a44fab89e08f8efd74201b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0e433bedec14fdf85420672fe56961f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "92511493fa9b4489967744e04133487d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06c90b350c1c44b79f66f790d6c00b15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "031e7b91ce7d4a2b89cd3eb12f78c57c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_845c7d69acf645f2a5a0e81ebdcce6ed",
              "IPY_MODEL_ebbe3bb2b0764865adf88f734460006d",
              "IPY_MODEL_9172c3a4e8d4470f98fa17ebdf80850a"
            ],
            "layout": "IPY_MODEL_dd1ad46f49fb4ffc904af6c9b1a2ddf5"
          }
        },
        "845c7d69acf645f2a5a0e81ebdcce6ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45343f8e8de74318948d316e752b6c64",
            "placeholder": "​",
            "style": "IPY_MODEL_4a417e4cced34764b9e5624c2ecd80b2",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ebbe3bb2b0764865adf88f734460006d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b963fb1ba68a4694b2267dba3c046c1f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb0a11f358674705aa05f9fbc7240cee",
            "value": 2
          }
        },
        "9172c3a4e8d4470f98fa17ebdf80850a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5c95c0921a044af95dfd89552e0e044",
            "placeholder": "​",
            "style": "IPY_MODEL_1c4dace00a9745ccaab531f1c26e946f",
            "value": " 2/2 [00:51&lt;00:00, 23.45s/it]"
          }
        },
        "dd1ad46f49fb4ffc904af6c9b1a2ddf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45343f8e8de74318948d316e752b6c64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a417e4cced34764b9e5624c2ecd80b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b963fb1ba68a4694b2267dba3c046c1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb0a11f358674705aa05f9fbc7240cee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5c95c0921a044af95dfd89552e0e044": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c4dace00a9745ccaab531f1c26e946f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}