{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svanalex/HaikusFromPaintings/blob/main/SelfEvalToolsV1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PII6PdzjVGjF"
      },
      "source": [
        "##Basic syllable counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujVVHPp-nh8u"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LMGu0Mh-yo0"
      },
      "outputs": [],
      "source": [
        "#building a better syllable counter\n",
        "import nltk\n",
        "nltk.download('cmudict')\n",
        "from nltk.corpus import cmudict\n",
        "d = cmudict.dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwPLv0IZMIwV"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import cmudict\n",
        "\n",
        "# Load the CMU Pronouncing Dictionary\n",
        "nltk.download('cmudict')\n",
        "cmu_dict = cmudict.dict()\n",
        "\n",
        "def estimate_syllables(word):\n",
        "    word = word.lower()\n",
        "    count = 0\n",
        "    vowels = \"aeiouy\"\n",
        "    if word and word[0] in vowels:\n",
        "        count += 1\n",
        "    for i in range(1, len(word)):\n",
        "        if word[i] in vowels and word[i - 1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith(\"e\"):\n",
        "        count -= 1\n",
        "    if count == 0:\n",
        "        count += 1\n",
        "    return count\n",
        "\n",
        "def syllable_count(word):\n",
        "    word = word.lower()\n",
        "    if word in cmu_dict:\n",
        "        # Take the first pronunciation\n",
        "        pronunciation = cmu_dict[word][0]\n",
        "        syllables = len([ph for ph in pronunciation if ph[-1].isdigit()])\n",
        "        return syllables\n",
        "    else:\n",
        "        return estimate_syllables(word)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtW1L4shmuXO"
      },
      "outputs": [],
      "source": [
        "def preprocess(sentence):\n",
        "  sentence = re.sub('[^A-Za-z0-9 ]+', '', sentence)\n",
        "  return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKdb8R6nk-eb"
      },
      "outputs": [],
      "source": [
        "def line_counter(sentence):\n",
        "  #need to make words from sentences\n",
        "  sentence = preprocess(sentence)\n",
        "  #print(sentence)\n",
        "  sentence_syllables = 0\n",
        "  words = sentence.split()\n",
        "  for word in words:\n",
        "    #print(word, syllable_count(word))\n",
        "    sentence_syllables += syllable_count(word)\n",
        "    #sentence_syllables += count_syllables(word)\n",
        "  return sentence_syllables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ymjb9EcVKqh"
      },
      "source": [
        "##Checking for haikus using syllable counter?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsJBPbeWYZ6V"
      },
      "outputs": [],
      "source": [
        "def syll_sentence(sentence):\n",
        "  syll_line = []\n",
        "  for line in sentence.split(\"\\n\"):\n",
        "    print(line_counter(line))\n",
        "    syll_line.append((line_counter(line), line))\n",
        "  return syll_line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phcl-HYbc4kQ"
      },
      "outputs": [],
      "source": [
        "def extract_flexible_haikus(syll_lines):\n",
        "    haikus = []\n",
        "\n",
        "    for i in range(1, len(syll_lines) - 1):\n",
        "        prev_syll, prev_line = syll_lines[i - 1]\n",
        "        curr_syll, curr_line = syll_lines[i]\n",
        "        next_syll, next_line = syll_lines[i + 1]\n",
        "\n",
        "        if 4 <= prev_syll <= 6 and 6 <= curr_syll <= 8 and 4 <= next_syll <= 6:\n",
        "            haikus.append((f\"{prev_line}\\n{curr_line}\\n{next_line}\", [prev_syll, curr_syll, next_syll]))\n",
        "            break\n",
        "    return haikus if haikus else [(\"no/improper haiku\", [0,0,0])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4twOZtEtekQV"
      },
      "outputs": [],
      "source": [
        "def score_haiku(haiku):\n",
        "  score = sum([x-y for x,y in zip([5,7,5], haiku[1])])\n",
        "  return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVlj3XQQVPiV"
      },
      "outputs": [],
      "source": [
        "test_sentence = \".\\n Here's the haiku:\\nMoonlight whispers\\nSilent brush strokes paint\\nDreaming midnight blue\\nThis haiku captures the essence of the painting's mood and style\"\n",
        "#test_sentence = \"\"\n",
        "sen = syll_sentence(test_sentence)\n",
        "haiku = extract_flexible_haikus(sen)\n",
        "print(haiku)\n",
        "print(haiku[0][0])\n",
        "print(score_haiku(haiku[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulp9agn0U40A"
      },
      "source": [
        "##Llama from hugging face as an eval tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAu9ptGCvEDZ"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVvys4eqsvKv"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DDndZkm1T9Z"
      },
      "outputs": [],
      "source": [
        "#text = f\"How would you rate this haiku on a scale from 1-10?: {haiku[0][0]}\"\n",
        "text = [f\"I need to grade this haiku for syllable count on a scale from 1-5. Treat this as a college level assignment and judge the quality appropriately: {haiku[0][0]}\",\n",
        "        f\"I need to grade this haiku for its relation to nature on a scale from 1-5. Treat this as a college level assignment and judge the quality appropriately: {haiku[0][0]}\",\n",
        "        f\"I need to grade this haiku for the quality of the creativity on a scale from 1-5. Treat this as a college level assignment and judge the quality appropriately: {haiku[0][0]}\",\n",
        "        f\"I need to grade this haiku for the imagery contained within it on a scale from 1-5. Treat this as a college level assignment and judge the quality appropriately: {haiku[0][0]}\"\n",
        "        ]\n",
        "\n",
        "\n",
        "for line in text:\n",
        "  inputs = tokenizer(line, return_tensors=\"pt\")\n",
        "  outputs = model.generate(**inputs, max_new_tokens=80)\n",
        "  print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxRXzpbhYcll"
      },
      "source": [
        "##BLIP as a self evaluation mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "240Sv0JzaLCn"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import AutoProcessor, BlipForImageTextRetrieval\n",
        "\n",
        "model = BlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n",
        "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "text = \"an image of a dog\"\n",
        "\n",
        "inputs = processor(images=image, text=text, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDTRAeegJrJJ"
      },
      "outputs": [],
      "source": [
        "#Sampled several HAIKU outputs for evaluation of iamge-haiku pairs. Have decided this is not an appropriate path forwards given incredibly low accuracy\n",
        "image = Image.open(\"/content/ce8b9ee46fa699101c2d31f2b4a9622e.jpg\")\n",
        "#text = \"painting of the moon and stars with a cityscape background\"\n",
        "#text = \"Golden dusk unfolds Shadows climb the quiet hills Night hums soft and slow\" #0.001\n",
        "#text = \"Black church towers rise Moonlight bleeds across the sky Sorrow echoes\" #0.222\n",
        "#text = \"A church stands stark Where moonlight bleeds soft shadows Silence whispers\" #0.002\n",
        "text = \"Lonely church stands tall Nocturnal shadows embrace Silence surrounds all\" #0.001\n",
        "text = \"Stark shadows drift Lights glimmer through bare branches Silence whispers cold\" #0.000\n",
        "text = \"Cherry tree's shadows Lights flicker between branches Silence whispers\" #0.000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3YZOLd1H1xc"
      },
      "outputs": [],
      "source": [
        "from transformers import BlipProcessor, BlipForImageTextRetrieval\n",
        "import torch\n",
        "\n",
        "# choose the ITM‑fine‑tuned checkpoint\n",
        "checkpoint = \"Salesforce/blip-itm-base-coco\"\n",
        "processor  = BlipProcessor.from_pretrained(checkpoint)\n",
        "model      = BlipForImageTextRetrieval.from_pretrained(checkpoint)\n",
        "\n",
        "# prepare your single image + single text\n",
        "inputs = processor(images=image, text=text, return_tensors=\"pt\")\n",
        "\n",
        "# forward pass: itm_logits has shape [1, 2]\n",
        "itm_out = model(**inputs)\n",
        "itm_logits = itm_out.itm_score\n",
        "\n",
        "# turn into probabilities [P(no‑match), P(match)]\n",
        "probs = itm_logits.softmax(dim=-1)\n",
        "\n",
        "match_prob = probs[0, 1].item()\n",
        "print(f\"P(image matches text) = {match_prob:.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SveWVFhwbLm-"
      },
      "outputs": [],
      "source": [
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUMSRC2Js8NX"
      },
      "source": [
        "##Trying other stuff with BLIP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPM_9S8ixH3x"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A34uBqklx0qA"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade numpy scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohjI2HEw3pBl"
      },
      "outputs": [],
      "source": [
        "#!pip install \"transformers==4.38.2\" \"numpy==1.23.5\"\n",
        "\n",
        "!pip install numpy==1.25.2 transformers==4.51.3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q08kbIJyw3QL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import AutoProcessor, Blip2ForImageTextRetrieval\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = Blip2ForImageTextRetrieval.from_pretrained(\"Salesforce/blip2-itm-vit-g\")\n",
        "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-itm-vit-g\")\n",
        "\n",
        "model.to(device)\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(\"/content/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\")\n",
        "text = \"Lonely church stands tall Nocturnal shadows embrace Silence surrounds all\"\n",
        "\n",
        "inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
        "itm_out = model(**inputs, use_image_text_matching_head=True)\n",
        "logits_per_image = torch.nn.functional.softmax(itm_out.logits_per_image, dim=1)\n",
        "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
        "\n",
        "print(f\"{probs[0][0]:.1%} that image 0 is not '{text}'\")\n",
        "print(f\"{probs[0][1]:.1%} that image 0 is '{text}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: nearly all haiku outputs are graded at ~26.9% accuracy. I have deemed that BLIP is fully inappropriate for this context due to this (at least given zero shot learning)"
      ],
      "metadata": {
        "id": "PuPHEK5zEBH6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M43_KC4PwPS"
      },
      "source": [
        "##Llama to text multiple haikus\n",
        "basically filtering 5 haikus to see if Llama can pick out a favorite/best haiku"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KS2fkiZDqc3l"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7vyv3VXQbFo"
      },
      "outputs": [],
      "source": [
        "#Llama-4\n",
        "#Note, this block remains completely unused as I was unable to get access to Llama 4\n",
        "from transformers import AutoProcessor, Llama4ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = Llama4ForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    attn_implementation=\"flex_attention\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "url1 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n",
        "url2 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/cat_style_layout.png\"\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"url\": url1},\n",
        "            {\"type\": \"image\", \"url\": url2},\n",
        "            {\"type\": \"text\", \"text\": \"Can you describe how these two images are similar, and how they differ?\"},\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "inputs = processor.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=True,\n",
        "    return_dict=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=256,\n",
        ")\n",
        "\n",
        "response = processor.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])[0]\n",
        "print(response)\n",
        "print(outputs[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#text = \"painting of the moon and stars with a cityscape background\"\n",
        "#text = \"Golden dusk unfolds Shadows climb the quiet hills Night hums soft and slow\" #0.001\n",
        "#text = \"Black church towers rise Moonlight bleeds across the sky Sorrow echoes\" #0.222\n",
        "#text = \"A church stands stark Where moonlight bleeds soft shadows Silence whispers\" #0.002\n",
        "text = \"Lonely church stands tall Nocturnal shadows embrace Silence surrounds all\" #0.001\n",
        "text = \"Stark shadows drift Lights glimmer through bare branches Silence whispers cold\" #0.000\n",
        "text = \"Cherry tree's shadows Lights flicker between branches Silence whispers\" #0.000"
      ],
      "metadata": {
        "id": "iJjFnf_nkzUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tCmxevSqJyF"
      },
      "outputs": [],
      "source": [
        "#LLama-3 70B\n",
        "#Model has OOM issues with 15GB of VRAM and 51GB of DRAM\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a professional haiku judge at a competition, who is tasked with evaluating three haikus and choosing a winner.\"},\n",
        "    {\"role\": \"user\", \"content\": \"A church stands stark Where moonlight bleeds soft shadows Silence whispers. Lonely church stands tall Nocturnal shadows embrace Silence surrounds all. Stark shadows drift Lights glimmer through bare branches Silence whispers cold.\"},\n",
        "]\n",
        "\n",
        "outputs = pipeline(\n",
        "    messages,\n",
        "    max_new_tokens=256,\n",
        ")\n",
        "print(outputs[0][\"generated_text\"][-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Okay, working implementation\n",
        "Llama 3.1-8B-instruct\n",
        "\n",
        "Powerful model for prompt engineering allowing for the system to be preprompted before a contextual user prompt is provided.\n",
        "\n",
        "This allows us to directly frame the model as a professional competition judge, and pass along multiple haikus, allowing the model to essentially self evaluate and choose a \"best\" haiku"
      ],
      "metadata": {
        "id": "_68miqRc6e5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "BEzlcsoK7r5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "eVRK7QrxEb1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "5rqReYNbxkyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "quantized_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "input_text = \"What are we having for dinner?\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "output = quantized_model.generate(**input_ids, max_new_tokens=10)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "Rwf2IEgtwsqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Want some basic evaluation. Capacity of the code to support realtime users is important!\n",
        "import time"
      ],
      "metadata": {
        "id": "4I_4ay_LtFGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic Evaluation Using Prompt Engineering\n",
        "t = time.time()\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a professional haiku judge at a competition, who is tasked with evaluating three haikus and choosing a winner.\"},\n",
        "    {\"role\": \"user\", \"content\": \"A church stands stark Where moonlight bleeds soft shadows Silence whispers. Lonely church stands tall Nocturnal shadows embrace Silence surrounds all. Stark shadows drift Lights glimmer through bare branches Silence whispers cold.\"},\n",
        "]\n",
        "\n",
        "input_ids = tokenizer.encode(messages[1]['content'], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = quantized_model.generate(input_ids, max_new_tokens=128)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "#print(outputs[0][\"generated_text\"][-1])\n",
        "print(f\"Time taken {time.time() - t}\")"
      ],
      "metadata": {
        "id": "iyA5KyiBsqGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Much more involved prompt engineering done below\n",
        "t = time.time()\n",
        "messages2 = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            \"You are a professional haiku competition judge. \"\n",
        "            \"Your job is to evaluate three haikus based on their poetic quality, emotional impact, imagery, and adherence to the traditional haiku structure. \"\n",
        "            \"After evaluating, choose the best haiku and explain your reasoning in 2-3 sentences.\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": (\n",
        "            \"Here are three haikus:\\n\\n\"\n",
        "            \"1. A church stands stark\\n\"\n",
        "            \"   Where moonlight bleeds soft shadows\\n\"\n",
        "            \"   Silence whispers.\\n\\n\"\n",
        "            \"2. Lonely church stands tall\\n\"\n",
        "            \"   Nocturnal shadows embrace\\n\"\n",
        "            \"   Silence surrounds all.\\n\\n\"\n",
        "            \"3. Stark shadows drift\\n\"\n",
        "            \"   Lights glimmer through bare branches\\n\"\n",
        "            \"   Silence whispers cold.\\n\\n\"\n",
        "            \"Please select the best one and explain why.\"\n",
        "        )\n",
        "    }\n",
        "]\n",
        "\n",
        "input_ids2 = tokenizer.encode(messages2[1]['content'], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs2 = quantized_model.generate(input_ids2, max_new_tokens=128)\n",
        "\n",
        "print(tokenizer.decode(outputs2[0], skip_special_tokens=True))\n",
        "print(f\"time taken {time.time() - t}\")"
      ],
      "metadata": {
        "id": "0I0VD9ShsqGJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPKM0f6xYv+AbKrnWCVSPqQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}